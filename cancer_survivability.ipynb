{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samuel/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../final_with_labels.csv\")\n",
    "data_matrix = data.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>pubcsnum</th>\n",
       "      <th>mar_stat</th>\n",
       "      <th>sex</th>\n",
       "      <th>primsite</th>\n",
       "      <th>histo3v</th>\n",
       "      <th>beho3v</th>\n",
       "      <th>grade</th>\n",
       "      <th>dx_conf</th>\n",
       "      <th>csexten</th>\n",
       "      <th>...</th>\n",
       "      <th>maligcount</th>\n",
       "      <th>benbordcount</th>\n",
       "      <th>year_dx</th>\n",
       "      <th>nhiade</th>\n",
       "      <th>codpub</th>\n",
       "      <th>srv_time_mon</th>\n",
       "      <th>srv_time_mon_flag</th>\n",
       "      <th>surv_1</th>\n",
       "      <th>surv_2</th>\n",
       "      <th>surv_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>7011365</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>C184</td>\n",
       "      <td>8210</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2010</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7014780</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>C209</td>\n",
       "      <td>8140</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "      <td>6</td>\n",
       "      <td>21040</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>7017183</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>C186</td>\n",
       "      <td>8140</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2012</td>\n",
       "      <td>6</td>\n",
       "      <td>21050</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>7017183</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>C209</td>\n",
       "      <td>8263</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>455</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2012</td>\n",
       "      <td>6</td>\n",
       "      <td>21050</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>7018423</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>C186</td>\n",
       "      <td>8263</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2011</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  pubcsnum  mar_stat  sex primsite  histo3v  beho3v  grade  dx_conf  \\\n",
       "0   1   7011365         9    2     C184     8210       3      2        1   \n",
       "1   2   7014780         5    2     C209     8140       3      2        1   \n",
       "2   3   7017183         2    1     C186     8140       2      9        1   \n",
       "3   4   7017183         2    1     C209     8263       3      2        1   \n",
       "4   5   7018423         2    1     C186     8263       3      3        1   \n",
       "\n",
       "   csexten   ...    maligcount  benbordcount  year_dx  nhiade  codpub  \\\n",
       "0      200   ...             4             0     2010       7       0   \n",
       "1      200   ...             2             0     2007       6   21040   \n",
       "2       50   ...             3             0     2012       6   21050   \n",
       "3      455   ...             3             0     2012       6   21050   \n",
       "4      400   ...             3             0     2011       2       0   \n",
       "\n",
       "   srv_time_mon  srv_time_mon_flag  surv_1  surv_2  surv_5  \n",
       "0            71                  1       1       1       1  \n",
       "1            36                  1       1       1       0  \n",
       "2            37                  1       1       1       0  \n",
       "3            37                  1       1       1       0  \n",
       "4            52                  1       1       1       1  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46785, 33)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y1 = data_matrix[:,-3]\n",
    "Y2 = data_matrix[:,-2]\n",
    "Y5 = data_matrix[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = data_matrix[:,2:24]\n",
    "X_cat = X[:,:-5]\n",
    "# primary site is only categorical variable with string labels\n",
    "X_primsite = X[:,2]\n",
    "X_cont = X[:,-5:]\n",
    "# X_cat contains only categorical variables with integer labels\n",
    "X_cat = np.delete(X_cat, [2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fill missing values in three eval columns with 9: unknown\n",
    "X_cat[:,9] = np.array([9 if np.isnan(x) else x for x in X_cat[:,9]])\n",
    "X_cat[:,10] = np.array([9 if np.isnan(x) else x for x in X_cat[:,10]])\n",
    "X_cat[:,11] = np.array([9 if np.isnan(x) else x for x in X_cat[:,11]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['C180', 'C181', 'C182', 'C183', 'C184', 'C185', 'C186', 'C187',\n",
       "       'C188', 'C189', 'C199', 'C209', 'C260'], dtype='<U4')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encode primary site\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "encoder = LabelBinarizer()\n",
    "X_primsite_1hot = encoder.fit_transform(X_primsite)\n",
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# one-hot encode remaining categorical variables\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "X_cat_1hot = encoder.fit_transform(X_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# combine all categorical variables\n",
    "X_cat = np.hstack((X_primsite_1hot, X_cat_1hot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The continuous variables in order: age_dx, eod10_pn, eod10_ne, cstumsiz, maligcount. Age and maligcount do not reuqire imputation, and eod10_ne (nodes examined) will not be used as a variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will impute values for eod10_pn and cstumsiz. We will implement multivariate imputation by chained equations (MICE) on these two continuous variables. Their indices are and values which need to be imputed are:\n",
    "\n",
    "1: eod10_pn (95, 97, 98, 99)<br>\n",
    "3: cstumsiz (998, 999)<br>\n",
    "\n",
    "Additionally, we will replace the values (991, 992, 993, 994, 995) in cstumsiz with (1, 2, 3, 4, 5) respectively.\n",
    "\n",
    "\n",
    "### Note: The above should be re-verified for each different dataset. This is for colorectal cancer in Hispanic patients after 2004."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cycle 1\n",
      "Cycle 2\n",
      "Cycle 3\n",
      "Cycle 4\n",
      "Cycle 5\n",
      "Cycle 6\n",
      "Cycle 7\n",
      "Cycle 8\n",
      "Cycle 9\n",
      "Cycle 10\n"
     ]
    }
   ],
   "source": [
    "# implements MICE imputation with linear regression model on eod10_pn and cstumsiz\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "NUM_CYCLES=10\n",
    "\n",
    "missing_values_pn = (95, 97, 98, 99)\n",
    "missing_values_tumsiz = (998, 999)\n",
    "\n",
    "pn = np.array(X_cont[:,1]).reshape(-1,1)\n",
    "tumsiz = np.array(X_cont[:,3]).reshape(-1,1)\n",
    "\n",
    "# non-MICE imputation\n",
    "for i, size in enumerate(tumsiz):\n",
    "    if size not in (990, 991, 992, 993, 994, 995):\n",
    "        continue\n",
    "    elif size == 990:\n",
    "        tumsiz[i] = 0\n",
    "    elif size == 991:\n",
    "        tumsiz[i] = 1\n",
    "    elif size == 992:\n",
    "        tumsiz[i] = 2\n",
    "    elif size == 993:\n",
    "        tumsiz[i] = 3\n",
    "    elif size == 994:\n",
    "        tumsiz[i] = 4\n",
    "    elif size == 995:\n",
    "        tumsiz[i] = 5\n",
    "\n",
    "\n",
    "# set missing values to -1\n",
    "for i, value in enumerate(pn):\n",
    "    if value in missing_values_pn:\n",
    "        pn[i] = -1\n",
    "for i, value in enumerate(tumsiz):\n",
    "    if value in missing_values_tumsiz:\n",
    "        tumsiz[i] = -1\n",
    "\n",
    "# indices for valid and imputation-needed positions in pn and tumsiz\n",
    "idx_pn_missing = np.where(pn == -1)[0]\n",
    "idx_pn_valid = np.where(pn != -1)[0]\n",
    "idx_tumsiz_missing = np.where(tumsiz == -1)[0]\n",
    "idx_tumsiz_valid = np.where(tumsiz != -1)[0]\n",
    "    \n",
    "# set missing values to mean\n",
    "# copy = false modifies array in place\n",
    "imp_pn = Imputer(missing_values = -1, copy=False)\n",
    "imp_pn.fit_transform(pn)\n",
    "imp_tumsiz = Imputer(missing_values = -1, copy=False)\n",
    "imp_tumsiz.fit_transform(tumsiz)\n",
    "\n",
    "\n",
    "for i in np.arange(NUM_CYCLES):\n",
    "    print(\"Cycle \" + str(i+1))\n",
    "    \n",
    "    # fit pn\n",
    "    X_ind = np.hstack((X_cat[idx_pn_valid], tumsiz[idx_pn_valid]))\n",
    "    X_rep = np.hstack((X_cat[idx_pn_missing], tumsiz[idx_pn_missing]))\n",
    "    lin_reg = LinearRegression()\n",
    "    lin_reg.fit(X_ind, pn[idx_pn_valid])\n",
    "    pn[idx_pn_missing] = lin_reg.predict(X_rep)\n",
    "    pn = np.maximum(pn, 0)\n",
    "    pn = np.minimum(pn, 100)\n",
    "    \n",
    "    # fit tumsiz\n",
    "    X_ind = np.hstack((X_cat[idx_tumsiz_valid], pn[idx_tumsiz_valid]))\n",
    "    X_rep = np.hstack((X_cat[idx_tumsiz_missing], pn[idx_tumsiz_missing]))\n",
    "    lin_reg = LinearRegression()\n",
    "    lin_reg.fit(X_ind, tumsiz[idx_tumsiz_valid])\n",
    "    tumsiz[idx_tumsiz_missing] = lin_reg.predict(X_rep)\n",
    "    tumsiz = np.maximum(tumsiz, 0)\n",
    "    tumsiz = np.minimum(tumsiz, 990)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pn = pn.reshape(-1,).astype(int)\n",
    "Counter(pn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(pn[idx_pn_valid]))\n",
    "print(np.std(pn[idx_pn_valid]))\n",
    "print(np.mean(pn[idx_pn_missing]))\n",
    "print(np.std(pn[idx_pn_missing]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(tumsiz[idx_tumsiz_valid]))\n",
    "print(np.std(tumsiz[idx_tumsiz_valid]))\n",
    "print(np.mean(tumsiz[idx_tumsiz_missing]))\n",
    "print(np.std(tumsiz[idx_tumsiz_missing]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(X_cont[:,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For hispanic colrect, there doesn't seem to be any categorical values in malig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46785, 313)\n"
     ]
    }
   ],
   "source": [
    "age_dx = np.array(X_cont[:,0])\n",
    "malig = np.array(X_cont[:,4])\n",
    "X_all = np.column_stack((X_cat, age_dx, pn, malig, tumsiz))\n",
    "print(X_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# shuffle\n",
    "np.random.seed(97)\n",
    "idx = np.random.permutation(len(X_all))\n",
    "X = X_all[idx]\n",
    "Y = Y5[idx]\n",
    "\n",
    "TEST_SET_SIZE = 6000\n",
    "\n",
    "X_train, X_test = X[:-TEST_SET_SIZE], X[-TEST_SET_SIZE:]\n",
    "Y_train, Y_test = Y[:-TEST_SET_SIZE].astype(int), Y[-TEST_SET_SIZE:].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samuel/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# feature scaling: scale features based on training data only\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "std_scaler = StandardScaler()\n",
    "X_train = std_scaler.fit_transform(X_train)\n",
    "#mm_scaler = MinMaxScaler()\n",
    "#X_train = mm_scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# min_sample_split: 300,400\n",
    "# min_samples_leaf: 200\n",
    "# max_depth: 130\n",
    "# min_weight_fraction_leaf: .01\n",
    "param_grid = [{'min_samples_leaf':[300,400,500]}]\n",
    "tree_clf_reg = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(tree_clf_reg, param_grid, cv=5, scoring=\"roc_auc\", verbose=1)\n",
    "grid_search.fit(X_train, Y_train.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(mean_score, params)\n",
    "print(\"Best: \" + str(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "Y_pred_test = grid_search.predict(std_scaler.transform(X_test))\n",
    "print(accuracy_score(Y_test.astype(int), Y_pred_test))\n",
    "print(\"ROC: \" + str(roc_auc_score(Y_test.astype(int), Y_pred_test)))\n",
    "confusion_matrix(Y_test.astype(int), Y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(X_train, Y_train.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "Y_pred_test = knn_clf.predict(std_scaler.transform(X_test))\n",
    "print(accuracy_score(Y_test.astype(int), Y_pred_test))\n",
    "print(\"ROC: \" + str(roc_auc_score(Y_test.astype(int), Y_pred_test)))\n",
    "confusion_matrix(Y_test.astype(int), Y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "param_grid = [{'penalty':['l2'], 'C':[0.1]}]\n",
    "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring=\"roc_auc\", n_jobs=-1, verbose=5)\n",
    "grid_search.fit(X_train, Y_train.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(mean_score, params)\n",
    "print(\"Best: \" + str(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "Y_pred_test = grid_search.predict(std_scaler.transform(X_test))\n",
    "print(accuracy_score(Y_test.astype(int), Y_pred_test))\n",
    "print(\"ROC: \" + str(roc_auc_score(Y_test.astype(int), Y_pred_test)))\n",
    "confusion_matrix(Y_test.astype(int), Y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=1000, n_estimators=500, n_jobs=-1, oob_score=False,\n",
       "         random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "        DecisionTreeClassifier(), n_estimators=500,\n",
    "        max_samples=1000, bootstrap=True, n_jobs=-1,\n",
    ")\n",
    "\n",
    "bag_clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probably add cross-validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samuel/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8246666666666667\n",
      "ROC: 0.7200112367929936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 746,  691],\n",
       "       [ 361, 4202]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "Y_pred_test = bag_clf.predict(std_scaler.transform(X_test))\n",
    "print(accuracy_score(Y_test.astype(int), Y_pred_test))\n",
    "print(\"ROC: \" + str(roc_auc_score(Y_test.astype(int), Y_pred_test)))\n",
    "confusion_matrix(Y_test.astype(int), Y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "accuracy_score(y_pred_rf, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf = Pipeline((\n",
    "                        (\"scaler\", StandardScaler()),\n",
    "                        (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
    "))\n",
    "svm_clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(svm_clf.predict(X_test), Y_test)\n",
    "accuracy_score(svm_clf.predict(X_train), Y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
